{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_d1Eg8XPIc6R"},"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'UOW/AISecurity/assignment2/'\n","FOLDERNAME = 'UOW/AISecurity/assignment2/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","sys.path.append('/content/drive/My Drive/{}/codebase'.format(FOLDERNAME))\n","\n","%cd /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lcw5pYBIgIC"},"source":["import torch\n","import sys\n","\n","device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n","\n","print('PyTorch Version:', torch.__version__)\n","print('-' * 60)\n","if torch.cuda.is_available():\n","    print('CUDA Device Count:', torch.cuda.device_count())\n","    print('CUDA Device Name:')\n","    for i in range(torch.cuda.device_count()):\n","        print('\\t', torch.cuda.get_device_name(i))\n","    print('CUDA Current Device Index:', torch.cuda.current_device())\n","    print('-' * 60)\n","\n","print(f\"Python version = {sys.version}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":["pdf-ignore"],"id":"U6xUQQMEIc6W"},"source":["# As usual, a bit of setup\n","import matplotlib.pyplot as plt\n","import types\n","from pathlib import Path\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","exp_cfg = types.SimpleNamespace()\n","exp_cfg.data_dir = Path(f\"/content/drive/My Drive/{FOLDERNAME}/data\")\n","exp_cfg.out_dir = Path(f\"/content/drive/My Drive/{FOLDERNAME}/out\")\n","\n","exp_cfg.data_dir.mkdir(parents=True, exist_ok=True)\n","exp_cfg.out_dir.mkdir(parents=True, exist_ok=True)\n","\n","exp_cfg.device = torch.device('cuda:0')  # use the first GPU\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Setup\n","Assignment 2 includes 3 tasks and 1 optional task:\n","- Task 1: Module Backdoor Attack (7 marks).\n","- Task 2: Reverse Engineering (7 marks).\n","- Task 3: Data-free Adaptive Attack against DeepJudge (6 marks).\n","- Optional Task: Image Watermarks (3 bonus marks).\n","\n","Please download a zip file from https://uowmailedu-my.sharepoint.com/:u:/g/personal/wzong_uow_edu_au/EXqNhJ2ncZhBlnZi3MCuOTwBc6yiKmK_zAdWdnWWf8JbBA?e=3zcClP\n","\n","Unzip the file and you will find 3 folders and 1 file:\n","- task1_model\n","- task2_model\n","- task3_model\n","- fingerprints.pt\n","\n","Upload all of them to the \"out\" folder in your assignment folder."],"metadata":{"id":"Z-u9LwlRdYuf"}},{"cell_type":"markdown","metadata":{"tags":["pdf-title"],"id":"eKK_KiZCIc6V"},"source":["# Task 1: Module Backdoor Attack (Total: 7 marks)\n","\n","In this task, you will implement module backdoor attack.\n","The target model is a clean VGG model trained on CIFAR10.\n","\n","\n","You need to train a module that will be attached to the target model to form a combined model.\n","The architecture of the module is provided.\n","\n","Following a similar strategy as TrojanNet, the output from this module is simply added to the output of the target model.\n","When a trigger exists in input images, the combined model outputs the predefined target label, i.e., 5 (dog).\n","Otherwise, the combined model behaves normally.\n","\n","The trigger is a red square at the bottom right corner of an image.\n","The height and width of the trigger are both 5.\n","The alpha value of the trigger is 1.0.\n","Recall that this trigger is the same as the \"large opaque\" trigger used in the lab of BadNets.\n","\n","Complete your code in train_module.py.\n","\n","Coding part marks:\n","- 5 marks if fooling rate >= 95% and decrease in accuracy <= 0.1%.\n","- 3 marks if fooling rate >= 70% and decrease in accuracy <= 0.1%.\n","- 2 marks if fooling rate >= 50% and decrease in accuracy <= 0.1%.\n","- 0 marks otherwise."]},{"cell_type":"markdown","source":["## Implement train in train_bad_module.py (5 marks)\n","Run the following cell to load the model for task 1."],"metadata":{"id":"fWh-kg-iKL4g"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","\n","import torchvision.transforms as transforms\n","from torchvision.datasets import CIFAR10\n","\n","from codebase import model_trainer, utils, setup\n","from codebase.classifiers import vgg\n","from codebase.datasets.poisoned import PoisonedDataset\n","\n","# load the pretrained task1 model\n","# input to this model must be normalized\n","cifar10_mean_tensor = torch.Tensor(setup.CIFAR10_MEAN).reshape([1, 3, 1, 1]).to(exp_cfg.device)\n","cifar10_std_tensor = torch.Tensor(setup.CIFAR10_STD).reshape([1, 3, 1, 1]).to(exp_cfg.device)\n","\n","dic_saved = model_trainer.ModelTrainer.load_latest_ckpt(exp_cfg.out_dir.joinpath(\"task1_model\"))\n","assert dic_saved is not None\n","task1_model = vgg.vgg11_bn(num_classes=10).to(exp_cfg.device)\n","task1_model.load_state_dict(dic_saved[\"model_state\"])\n","task1_model.eval()\n","\n","# clean testing set without triggers\n","clean_test_set = CIFAR10(root=str(exp_cfg.data_dir), train=False, download=True,\n","                            transform=transforms.Compose([\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(setup.CIFAR10_MEAN, setup.CIFAR10_STD)\n","                            ]))\n","\n","# evaluate it on the clean testing set\n","_, org_clean_acc, _ = model_trainer.ModelTrainer.eval_on_dset(task1_model, clean_test_set)\n"],"metadata":{"id":"Srvg0wFNJ553"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the following cell to create a poisoned testing set and visualize some trojaned images."],"metadata":{"id":"84UuK7gAK6sV"}},{"cell_type":"code","source":["# create a poisoned dataset\n","# triggers are superimposed on raw images with uint8 pixel values from [0, 255]\n","poison_target = 5        # dog\n","trigger_size = 5\n","trigger_alpha = 1.0\n","trigger = np.zeros([trigger_size, trigger_size, 3], dtype=np.uint8)\n","trigger[:, :, 0] = 255\n","\n","IMAGE_SIZE = 32\n","trigger_loc = [IMAGE_SIZE - trigger_size, IMAGE_SIZE - trigger_size]\n","\n","# make sure that the trigger is inside the image\n","assert (trigger.shape[0] + trigger_loc[0] <= IMAGE_SIZE) and (trigger.shape[1] + trigger_loc[1] <= IMAGE_SIZE)\n","\n","# poisoned testing set with a trigger added to each image.\n","poisoned_test_set = PoisonedDataset(\n","    clean_dset=CIFAR10(root=str(exp_cfg.data_dir), train=False, download=True,\n","                        transform=transforms.Compose([\n","                            transforms.ToTensor(),\n","                            transforms.Normalize(setup.CIFAR10_MEAN, setup.CIFAR10_STD)\n","                        ])),\n","    poison_rate=1.0, poison_target=poison_target, trigger=trigger,\n","    trigger_loc=trigger_loc,\n","    trigger_alpha=trigger_alpha, poison_seed=375975,\n","    only_extract_poisoned=True,  # only calculate success rates on poisoned data\n",")\n","\n","\n","# also evaluate the model on the poisoned testing set\n","# the resulting attack success rate should be close to 0 because this model is not trojaned.\n","model_trainer.ModelTrainer.eval_on_dset(task1_model, poisoned_test_set)\n","\n","# We now visualize examples of clean and poisoned images.\n","\n","vis_num = 5\n","vis_img_idx_lst = np.random.RandomState(seed=3752).permutation(len(clean_test_set))[:vis_num]     # number of images to visualize\n","\n","# Visualize clean images and poisoned images.\n","fig, axs = plt.subplots(nrows=2, ncols=vis_num, figsize=(10, 5))\n","axs[0, 0].set_ylabel(\"clean image\")\n","axs[1, 0].set_ylabel(\"poisoned image\")\n","\n","for dset_idx, dset in enumerate([clean_test_set, poisoned_test_set]):\n","    for vis_idx, img_idx in enumerate(vis_img_idx_lst):\n","        x, y = dset[img_idx]\n","        x = x.unsqueeze(0).to(exp_cfg.device)\n","\n","        # images are normalized. Need to unnormalize them first and then change pixel values to [0, 255] for visualization\n","        x = utils.unnormalize(x, cifar10_mean_tensor, cifar10_std_tensor)\n","        x = (x * 255).detach().cpu().squeeze(0).numpy().astype(np.uint8).transpose([1, 2, 0])\n","\n","        axs[dset_idx, vis_idx].set_xlabel(f\"{setup.CIFAR10_CLASSES[y]}\")\n","        axs[dset_idx, vis_idx].imshow(x)\n","\n","        axs[dset_idx, vis_idx].get_xaxis().set_ticks([])\n","        axs[dset_idx, vis_idx].get_yaxis().set_ticks([])\n","\n","plt.tight_layout()\n","plt.show()\n","plt.close()\n"],"metadata":{"id":"Xkc7IibHK-5G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the following cell to define a simple architecture for your module."],"metadata":{"id":"zDJn6YdcLek4"}},{"cell_type":"code","source":["class SimpleConvNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(256, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.25),\n","\n","            nn.Linear(64, num_classes),\n","        )\n","\n","    def forward(self, x, return_features=False):\n","        features = self.features(x)\n","\n","        out = features.view(features.size(0), -1)\n","        out = self.classifier(out)\n","\n","        if return_features is True:\n","            return out, features\n","\n","        return out"],"metadata":{"id":"e76hVOMgLfBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement your train function in train_bad_module.py. Run the cell below to evaluate your implementation and print out your marks for this coding part."],"metadata":{"id":"031Adpn6L4wU"}},{"cell_type":"code","source":["import train_bad_module as train_bad_module\n","\n","bad_module = SimpleConvNet(10)\n","\n","task1_params = sum(p.numel() for p in task1_model.parameters() if p.requires_grad)\n","print(f\"task1_model has {task1_params} parameters.\")\n","\n","bad_params = sum(p.numel() for p in bad_module.parameters() if p.requires_grad)\n","print(f\"bad_module has {bad_params} parameters. (equal to {bad_params / task1_params *100:.2f}% task1_model parameters.)\")\n","\n","bad_module = bad_module.to(exp_cfg.device)\n","train_bad_module.train(bad_module=bad_module, poison_target=poison_target,\n","                        trigger_size=trigger_size, trigger_alpha=trigger_alpha,\n","                        exp_cfg=exp_cfg\n","                        )\n","\n","task1_model.eval()\n","bad_module.eval()\n","\n","# this model combines output from the target model and the bad_module.\n","class CombinedModel(nn.Module):\n","    def __init__(self, model1, model2, alpha_1, alpha_2):\n","        super().__init__()\n","        self.model1 = model1\n","        self.model2 = model2\n","        self.alpha_1 = alpha_1\n","        self.alpha_2 = alpha_2\n","\n","    def forward(self, x):\n","        return self.model1(x) * self.alpha_1 + self.model2(x) * self.alpha_2\n","\n","combined_model = CombinedModel(task1_model, bad_module, 1.0, 5.0)\n","combined_model.eval()\n","\n","# evaluate accuracy on clean testing data\n","_, task1_combined_clean_acc, _ = model_trainer.ModelTrainer.eval_on_dset(combined_model, clean_test_set)\n","\n","# evaluate the attack success rate on poisoned data.\n","_, task1_attack_success_rate, _ = model_trainer.ModelTrainer.eval_on_dset(combined_model, poisoned_test_set)\n","\n","task1_marks = 0\n","if task1_combined_clean_acc < org_clean_acc - 0.001:\n","    print(\"The drop in accuracy is too large.\")\n","else:\n","    if task1_attack_success_rate >= 0.95:\n","        task1_marks = 5\n","    elif task1_attack_success_rate >= 0.70:\n","        task1_marks = 3\n","    elif task1_attack_success_rate >= 0.50:\n","        task1_marks = 2\n","\n","print(f\"\\n******* Your marks for Task 1 coding part is {task1_marks}/5. *******\\n\")\n"],"metadata":{"id":"JklSPp3aL5LN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Briefly describe how you implement the module backdoor attack. (2 marks)\n","\n","Your answer:"],"metadata":{"id":"iYJLnorkHbU3"}},{"cell_type":"markdown","source":["# Task 2: Reverse Engineering (Total: 7 marks)\n","\n","In this task, you will reverse engineer the embedded trigger.\n","The target model is a poisoned VGG model.\n","The target label is 5 (dog).\n","\n","In your implementation, you need to reverse engineer the trigger using a clean CIFAR10 training set.\n","The reversed trigger should be able to fool the model effectively when blended with clean images.\n","A mask is used as alpha values in the blending operation.\n","\n","Your reversed trigger should look reasonably similar to the original trigger.\n","It is acceptable that the reversed trigger appears at a different location compared to the original trigger.\n","For example, the reversed trigger may look horizontally flipped compared to the original trigger.\n","This is because RandomHorizontalFlip was applied during the training of the trojaned model.\n","Triggers may also appear at other locations since the target model can learn multiple effective triggers during training.\n","Ideally, the reversed trigger should be in the bottom area.\n","Due to the unpredictability of optimization, the reversed trigger varies for each run.\n","\n","Complete your code in reverse_engineer.py.\n","\n","Coding part marks:\n","- 5 marks if fooling rate of the reversed trigger >= 95%.\n","- 3 marks if fooling rate of the reversed trigger >= 70%.\n","- 2 marks if fooling rate of the reversed trigger >= 50%.\n","- 0 marks otherwise.\n","\n","Note that if your reversed trigger is significantly different from the original trigger, e.g., your reversed trigger looks like random noise, this means you fail to reverse the trigger and your marks will be manually changed to 0 for this coding part.\n","\n","Run the following cell to show examples of successfully reversed triggers.\n","You can find more examples in the \"images\" folder."],"metadata":{"id":"vwOTMCMuIhfk"}},{"cell_type":"code","source":["import matplotlib.image as mpimg\n","\n","example_path_lst = [\n","    \"images/reversed_trigger_2.png\", \"images/reversed_trigger_4.png\",\n","]\n","\n","fig, axs = plt.subplots(nrows=1, ncols=len(example_path_lst), figsize=(10, 10))\n","\n","\n","for vis_idx, example_path in enumerate(example_path_lst):\n","    img = mpimg.imread(exp_cfg.out_dir.parent.joinpath(example_path))\n","\n","    axs[vis_idx].set_xlabel(f\"Example {vis_idx+1}\")\n","    axs[vis_idx].imshow(img)\n","\n","    axs[vis_idx].get_xaxis().set_ticks([])\n","    axs[vis_idx].get_yaxis().set_ticks([])\n","\n","plt.tight_layout()\n","plt.show()\n","plt.close()"],"metadata":{"id":"ZDXBwT5zSpMU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implement reverse_trigger in reverse_engineer.py (5 marks)\n","Run the following cell to load the model for task 2."],"metadata":{"id":"5JHO3c4tTBaI"}},{"cell_type":"code","source":["# load the task 2 model\n","dic_saved = model_trainer.ModelTrainer.load_latest_ckpt(exp_cfg.out_dir.joinpath(\"task2_model\"))\n","assert dic_saved is not None\n","task2_model = vgg.vgg11_bn(num_classes=10).to(exp_cfg.device)\n","task2_model.load_state_dict(dic_saved[\"model_state\"])\n","task2_model.eval()\n","\n","# evaluate it on the clean and poisoned testing sets\n","# clean_test_set and poisoned_test_set are already defined in cells above\n","model_trainer.ModelTrainer.eval_on_dset(task2_model, clean_test_set)\n","model_trainer.ModelTrainer.eval_on_dset(task2_model, poisoned_test_set)\n"],"metadata":{"id":"PeLFa5DLTpFi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement your reverse_trigger function in reverse_engineer.py. Run the cell below to evaluate your implementation and print out your marks for this coding part."],"metadata":{"id":"8PzUkCCiddAx"}},{"cell_type":"code","source":["import reverse_engineer as reverse_engineer\n","\n","reversed_trigger, trigger_mask = reverse_engineer.reverse_trigger(task2_model, poison_target, exp_cfg)\n","assert reversed_trigger.dtype == np.uint8 and trigger_mask.dtype == np.float32\n","\n","# poisoned testing set with a trigger added to each image.\n","masked_poisoned_test_set = PoisonedDataset(\n","    clean_dset=CIFAR10(root=str(exp_cfg.data_dir), train=False, download=True,\n","                        transform=transforms.Compose([\n","                            transforms.ToTensor(),\n","                            transforms.Normalize(setup.CIFAR10_MEAN, setup.CIFAR10_STD)\n","                        ])),\n","    poison_rate=1.0, poison_target=poison_target, trigger=reversed_trigger, trigger_alpha=None,\n","    trigger_loc=None, trigger_mask=trigger_mask,\n","    poison_seed=375975, only_extract_poisoned=True,  # only calculate success rates on poisoned data\n",")\n","\n","_, task2_attack_success_rate, _ = model_trainer.ModelTrainer.eval_on_dset(task2_model, masked_poisoned_test_set)\n","\n","# calculate the masked trigger that is superimposed on images.\n","masked_trigger = reversed_trigger * trigger_mask.reshape(*trigger_mask.shape, 1)\n","masked_trigger = masked_trigger.astype(np.uint8)\n","\n","# when training the trojaned model, input images are randomly flipped along the horizontal axis\n","# therefore, the reversed trigger may also be flipped horizontally.\n","masked_trigger_flipped = np.flip(masked_trigger, axis=1)\n","\n","# the ground truth trigger is a 5*5 red square at the bottom right corner\n","ground_truth_trigger = np.zeros([32, 32, 3], dtype=np.uint8)\n","ground_truth_trigger[27:, 27:, 0] = 255\n","\n","# compare the difference between the masked trigger and the ground truth trigger\n","fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(5, 5))\n","axs[0, 0].set_ylabel(r\"Triggers\")\n","axs[1, 0].set_ylabel(r\"Clean images\")\n","axs[2, 0].set_ylabel(r\"Trojaned images\")\n","\n","axs[0, 0].set_xlabel(f\"Ground truth trigger\")\n","axs[0, 0].imshow(ground_truth_trigger)\n","\n","axs[0, 1].set_xlabel(f\"Reversed trigger\")\n","axs[0, 1].imshow(masked_trigger)\n","\n","axs[0, 2].set_xlabel(f\"Reversed trigger flipped\")\n","axs[0, 2].imshow(masked_trigger_flipped)\n","\n","for i in range(3):\n","    axs[0, i].get_xaxis().set_ticks([])\n","    axs[0, i].get_yaxis().set_ticks([])\n","\n","for dset_idx, dset in enumerate([clean_test_set, masked_poisoned_test_set]):\n","    for vis_idx, img_idx in enumerate([0, 1, 2]):\n","        if dset == clean_test_set:\n","            # show the same images for clean and trojaned images.\n","            img_idx = masked_poisoned_test_set.poison_idx_lst[img_idx]\n","\n","        x, y = dset[img_idx]\n","        x = x.unsqueeze(0).to(exp_cfg.device)\n","\n","        # images are normalized. Need to unnormalize them first and then change pixel values to [0, 255] for visualization\n","        x = utils.unnormalize(x, cifar10_mean_tensor, cifar10_std_tensor)\n","        x = (x * 255).detach().cpu().squeeze(0).numpy().astype(np.uint8).transpose([1, 2, 0])\n","\n","        axs[dset_idx + 1, vis_idx].set_xlabel(f\"{setup.CIFAR10_CLASSES[y]}\")\n","        axs[dset_idx + 1, vis_idx].imshow(x)\n","\n","        axs[dset_idx + 1, vis_idx].get_xaxis().set_ticks([])\n","        axs[dset_idx + 1, vis_idx].get_yaxis().set_ticks([])\n","\n","plt.tight_layout()\n","plt.show()\n","plt.close()\n","\n","task2_marks = 0\n","\n","if task2_attack_success_rate >= 0.95:\n","    task2_marks = 5\n","elif task2_attack_success_rate >= 0.70:\n","    task2_marks = 3\n","elif task2_attack_success_rate >= 0.50:\n","    task2_marks = 2\n","\n","print(f\"\\n******* Your marks for Task 2 coding part is {task2_marks}/5. *******\\n\")"],"metadata":{"id":"ewn08CHTddYZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Briefly describe how you reverse engineer the trigger. (2 marks)\n","\n","Your answer:"],"metadata":{"id":"XnHfkJ-YLoj7"}},{"cell_type":"markdown","source":["# Task 3: Data-free Adaptive Attack against DeepJudge (Total: 6 marks)\n","\n","In this task, you will implement an adaptive attack against DeepJudge.\n","DeepJudge exploits fingerprints to protect the intellectual property of a target model.\n","\n","This task considers a data-free scenario, in which you cannot access any training or testing data.\n","You need to implement a preprocessing function which reasonably transforms input images before passing them to the target model.\n","Your preprocessing function is expected to make DeepJudge ineffective while preserving the performance of the target model.\n","\n","Complete your transform function in adaptive_attack.py.\n","\n","Coding part marks:\n","- 5 marks if decrease in accuracy < 0.5% and defeating DeepJudge.\n","- 3 marks if decrease in accuracy < 1.5% and defeating DeepJudge.\n","- 2 marks if decrease in accuracy < 3.0% and defeating DeepJudge.\n","- 0 marks otherwise."],"metadata":{"id":"5p3YaP1Ghdkd"}},{"cell_type":"markdown","source":["Run the following code to load the model for task 3 and set up DeepJudge."],"metadata":{"id":"xej7d4VHhjXs"}},{"cell_type":"code","source":["# calculating Rob and JSD.\n","\n","def Rob(model, advx, advy):\n","    \"\"\" Robustness (empirical)\n","    args:\n","        model: suspect model\n","        advx: black-box test cases (adversarial examples)\n","        advy: ground-truth labels\n","\n","    return:\n","        Rob value\n","    \"\"\"\n","    model.eval()\n","    out_logts = model(advx).cpu().detach().numpy()\n","    advy = advy.cpu().detach().numpy()\n","\n","    return np.sum(np.argmax(out_logts, axis=1) == advy) / advy.shape[0]\n","\n","def JSD(model1, model2, advx):\n","    \"\"\" Jensen-Shanon Distance\n","    args:\n","        model1 & model2: victim model and suspect model\n","        advx: black-box test cases\n","\n","    return:\n","        JSD value\n","    \"\"\"\n","    model1.eval()\n","    model2.eval()\n","\n","    vectors1 = torch.softmax(model1(advx), dim=1).cpu().detach().numpy() + 1e-8\n","    vectors2 = torch.softmax(model2(advx), dim=1).cpu().detach().numpy() + 1e-8\n","    mid = (vectors1 + vectors2) / 2\n","    distances = (scipy.stats.entropy(vectors1, mid, axis=1) + scipy.stats.entropy(vectors2, mid, axis=1)) / 2\n","    return np.average(distances)\n","\n","def cal_Rob_in_batch(model, advx, advy):\n","    batch_size = 128\n","    batch_num = advx.shape[0] // batch_size\n","    if advx.shape[0] % batch_size != 0:\n","        batch_num += 1\n","\n","    total_rob = 0.0\n","    for cur_batch in range(batch_num):\n","        batch_x = advx[cur_batch * batch_size: (cur_batch + 1) * batch_size]\n","        batch_y = advy[cur_batch * batch_size: (cur_batch + 1) * batch_size]\n","\n","        batch_rob = Rob(model, batch_x, batch_y)\n","        total_rob += (batch_rob * batch_y.shape[0])\n","\n","    return total_rob / advy.shape[0]\n","\n","def cal_JSD_in_batch(model1, model2, advx):\n","    batch_size = 128\n","    batch_num = advx.shape[0] // batch_size\n","    if advx.shape[0] % batch_size != 0:\n","        batch_num += 1\n","\n","    total_jsd = 0.0\n","    for cur_batch in range(batch_num):\n","        batch_x = advx[cur_batch * batch_size: (cur_batch + 1) * batch_size]\n","\n","        batch_jsd = JSD(model1, model2, batch_x)\n","        total_jsd += (batch_jsd * batch_x.shape[0])\n","\n","    return total_jsd / advx.shape[0]\n","\n","# load the task 3 model\n","dic_saved = model_trainer.ModelTrainer.load_latest_ckpt(exp_cfg.out_dir.joinpath(\"task3_model\"))\n","assert dic_saved is not None\n","task3_model = vgg.vgg11_bn(num_classes=10).to(exp_cfg.device)\n","task3_model.load_state_dict(dic_saved[\"model_state\"])\n","task3_model.eval()\n","\n","_, task3_clean_acc, _ = model_trainer.ModelTrainer.eval_on_dset(task3_model, clean_test_set)\n","\n","# load DeepJudge fingerprints\n","fprint_x, fprint_y = torch.load(exp_cfg.out_dir.joinpath(\"fingerprints.pt\"))\n","\n","assert np.isclose(cal_Rob_in_batch(task3_model, fprint_x, fprint_y),\n","                    0.0), \"task3_model must have 0 ROBD as fingerprints are generated by it.\"\n","assert np.isclose(cal_JSD_in_batch(task3_model, task3_model, fprint_x),\n","                    0.0), \"task3_model must have 0 JSD as fingerprints are generated by it.\"\n","\n","# the thresholds for detecting IP infringement\n","robd_ttest_thres = 0.388\n","jsd_ttest_thres = 0.226\n"],"metadata":{"id":"s4IZY-e8hjy-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implement transform function in adaptive_attack.py  (5 marks)\n","Run the cell below to evaluate your implementation and print out your marks for this coding part."],"metadata":{"id":"VppnIFWWpoP0"}},{"cell_type":"code","source":["import adaptive_attack as adaptive_attack\n","\n","# your preprocessing function is called in forward.\n","class DefeatJeepJudge(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","\n","    def forward(self, x):\n","        x = adaptive_attack.transform(x, exp_cfg)\n","        return self.model(x)\n","\n","defeat_deepjudge = DefeatJeepJudge(task3_model)\n","defeat_deepjudge.eval()\n","\n","_, defeat_deepjudge_acc, _ = model_trainer.ModelTrainer.eval_on_dset(defeat_deepjudge, clean_test_set)\n","\n","with torch.no_grad():\n","    # calculate robds and jsd\n","    robd = cal_Rob_in_batch(defeat_deepjudge, fprint_x, fprint_y)\n","    jsd = cal_JSD_in_batch(defeat_deepjudge, task3_model, fprint_x)\n","\n","print(f\"robd = {robd:.3f}; jsd = {jsd:.3f}\")\n","\n","# whether IP infringement is detected\n","detected = True if (robd < robd_ttest_thres) or (jsd < jsd_ttest_thres) else False\n","\n","task3_marks = 0\n","\n","if detected is True:\n","    print(\"Fail to defeat DeepJudge.\")\n","\n","else:\n","    if defeat_deepjudge_acc > task3_clean_acc - 0.005:\n","        task3_marks = 5\n","\n","    elif defeat_deepjudge_acc > task3_clean_acc - 0.015:\n","        task3_marks = 3\n","\n","    elif defeat_deepjudge_acc > task3_clean_acc - 0.03:\n","        task3_marks = 2\n","\n","print(f\"\\n******* Your marks for Task 3 coding part is {task3_marks}/5. *******\\n\")"],"metadata":{"id":"UcX1uwYbpooQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Explain the rationale behind your adaptive attack. (1 mark)\n","\n","Your answer:"],"metadata":{"id":"df_Npf3iqt5R"}},{"cell_type":"markdown","source":["# Total marks for the coding part\n","Run the cell below to calculate your marks."],"metadata":{"id":"Sf1C2Cv-rwpJ"}},{"cell_type":"code","source":["print(f\"\\n *******Your total marks for assignment 2 coding part is {task1_marks + task2_marks + task3_marks}/15. *******\\n\")"],"metadata":{"id":"YzB1ZqR99vIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Optional Task: Image Watermarks (3 bonus marks)\n","\n","In this task, you will implement a deep watermarking technique that embeds subtle watermarks to clean images.\n","You are given an **autoencoder** and a watermark **decoder**:\n","- The **autoencoder** aims to generate watermarks (i.e. perturbations) that will be added to clean images.\n","- The **decoder** aims to recover predefined watermarks from watermarked images. If input images are not watermarked (i.e., clean images), the **decoder** returns a squence of 0.\n","\n","Complete your train function in watermark.py.\n","\n","Marks are awarded based on True Positive Rate (TPR), True Negative Rate (TNR) and False Positive Rate (FPR):\n","- 2 marks if TPR >= 95% and TNR >= 95% and FPR <= 5%.\n","- 0 marks otherwise.\n","\n","Please read the code and comments to see how these metrics are calculated.\n","\n","It is acceptable that your watermarks are **slightly visible**.\n","However, if your watermarks are **obvious noise**, this means you failed to generate subtle watermarks and your marks will be 0 for this coding part.\n","\n","Hint: if your FPR is very large, e.g., close to 100%, this usually means your watermarks are too small to be detected by the watermark decoder. You may need to increase your watermark strength.\n","\n","Run the following cell to load examples of acceptable watermarks that achieved full marks.\n","You can find more examples in the \"images\" folder."],"metadata":{"id":"gikTSLTmN0Or"}},{"cell_type":"code","source":["import matplotlib.image as mpimg\n","\n","plt.figure()\n","img = mpimg.imread(exp_cfg.out_dir.parent.joinpath(\"images/acceptable_wm_2.png\"))\n","plt.imshow(img)\n","\n","ax = plt.gca()\n","ax.set_xticks([])\n","ax.set_yticks([])\n","\n","plt.tight_layout()\n","plt.show()\n","plt.close()"],"metadata":{"id":"nU4h1385wsVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implement train in watermark.py (2 marks)\n","\n","Run the following cell to define the watermark decoder. Do not be confused by this watermark decoder and the decoder contained in the autoencoder:\n","- This watermark decoder aims to recover watermark bits, i.e., \"1,0,1,0,1...\".\n","- The decoder in the autoencoder aims to generate perturbations that will be added to clean images."],"metadata":{"id":"PUXCU3-1Vwwz"}},{"cell_type":"code","source":["# this watermark decoder is a simple convolutional neural network.\n","class WatermarkDecoder(nn.Module):\n","    def __init__(self, wm_len):\n","        super().__init__()\n","\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(1024, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.20),\n","            nn.Linear(256, wm_len),\n","        )\n","\n","    def forward(self, x):\n","        features = self.features(x)\n","\n","        out = features.view(features.size(0), -1)\n","        out = self.classifier(out)\n","\n","        return out"],"metadata":{"id":"hWYJEHPyaLf2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the following cell to create the training dataset."],"metadata":{"id":"Fmy2pdn5asIh"}},{"cell_type":"code","source":["# clean training set\n","clean_train_set = CIFAR10(root=str(exp_cfg.data_dir), train=True, download=True,\n","                            transform=transforms.Compose([\n","                                transforms.ToTensor(),\n","                                transforms.RandomHorizontalFlip(0.5),\n","                                transforms.Normalize(setup.CIFAR10_MEAN, setup.CIFAR10_STD)\n","                            ])\n","                         )\n","print(f\"Size of the training set = {len(clean_train_set)}\")"],"metadata":{"id":"l-AJz2LxaxIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the following cell to train models and evaluate your watermarks on the testing dataset.\n","You can read how the autoencoder is implemented in AutoEncoder.py."],"metadata":{"id":"AnUaXhc1btqm"}},{"cell_type":"code","source":["import watermark as watermark\n","from codebase.autoencoder.AutoEncoder import AutoEncoder\n","\n","# the watermark to embed\n","wm = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int64)\n","\n","# the configuration for the autoencoder\n","encoder_cfg = [32, \"M\", 64, \"M\", 64, \"M\"]\n","decoder_cfg = [\"M\", 64, \"M\", 32, \"M\", 3]\n","in_shape = [3, 32, 32]\n","fc_hidden_dim = 512\n","latent_dim = 128\n","\n","# create autoencoder and watermark decoder and train them from scratch\n","ae = AutoEncoder(encoder_cfg, decoder_cfg, in_shape, fc_hidden_dim, latent_dim, wm=wm)\n","ae = ae.to(exp_cfg.device)\n","decoder = WatermarkDecoder(wm.shape[0]).to(exp_cfg.device)\n","\n","watermark.train(ae=ae, decoder=decoder, wm=wm, dset=clean_train_set, exp_cfg=exp_cfg)\n","\n","ae.eval()\n","decoder.eval()\n","\n","# use your trained autoencoder to embed watermarks into the testing data and try to decode them\n","clean_test_loader = torch.utils.data.DataLoader(clean_test_set, batch_size=128, num_workers=8, shuffle=False)\n","\n","# plot some images for visualization\n","num_vis = 10\n","for test_x, test_y in clean_test_loader:\n","    test_x = test_x[:num_vis].to(exp_cfg.device)\n","    test_y = test_y[:num_vis]\n","\n","    # add watermarks to test_x\n","    wm_test_x = test_x + ae(test_x)\n","\n","    # concatenate them together and visualize them\n","    vis_x = torch.concatenate([test_x, wm_test_x], dim=0)\n","    test_y = torch.tile(test_y, (2,))\n","\n","    vis_x = utils.unnormalize(vis_x, cifar10_mean_tensor, cifar10_std_tensor)\n","\n","    utils.show_imgs_tensor(nrows=2, ncols=num_vis, imgs_arr=vis_x, labels_arr=test_y,\n","                            class_names=setup.CIFAR10_CLASSES,\n","                            ylabels=[\"Clean\", \"Watermarked\"])\n","\n","    break\n","\n","# we consider the following metrics:\n","# true positive rate (TPR) = TP / (TP+FN)\n","# false positive rate (FPR) = FP / (FP+TN)\n","# true negative rate (TNR) = TN / (TN+FP)\n","tp_arr = []\n","fp_arr = []\n","tn_arr = []\n","\n","# watermarked images should be decoded as wm\n","wm_tensor = torch.from_numpy(wm).long().to(exp_cfg.device)\n","wm_tensor = wm_tensor.unsqueeze(0)\n","# clean images should be decoded as all 0s\n","clean_wm_tensor = torch.zeros_like(wm_tensor).long()\n","\n","# calculate detection metrics for all the testing images\n","for test_x, test_y in clean_test_loader:\n","    test_x = test_x.to(exp_cfg.device)\n","\n","    # add watermarks to test_x\n","    wm_test_x = test_x + ae(test_x)\n","\n","    def do_decode(_x, _target):\n","        decoded_x = torch.sigmoid(decoder(_x))\n","        decoded_x = (decoded_x > 0.5).long()\n","\n","        # watermarks are identified only if all the bits are correct\n","        correct = (decoded_x == _target)\n","        correct = correct.sum(dim=1)\n","        correct = (correct == wm.shape[0])\n","\n","        return correct\n","\n","    # true positives mean watermarks are successfully recovered from watermarked images\n","    tp_arr.append(do_decode(wm_test_x, wm_tensor))\n","    # false positives mean watermarks are incorrectly recovered from clean images\n","    fp_arr.append(do_decode(test_x, wm_tensor))\n","    # true negatives mean only bits 0 are obtained from clean images result\n","    tn_arr.append(do_decode(test_x, clean_wm_tensor))\n","\n","assert sum(x.shape[0] for x in tp_arr) == len(clean_test_set), \"All the testing data must be included.\"\n","\n","tpr = torch.concatenate(tp_arr, dim=0).cpu().detach().numpy().mean()\n","fpr = torch.concatenate(fp_arr, dim=0).cpu().detach().numpy().mean()\n","tnr = torch.concatenate(tn_arr, dim=0).cpu().detach().numpy().mean()\n","\n","print(f\"TPR = {tpr*100.0:.1f}%, FPR = {fpr*100.0:.1f}%, TNR = {tnr*100.0:.1f}%\")\n","\n","# calculate marks\n","task4_marks = 0\n","if fpr >= 0.05:\n","    print(f\"Your FPR is too large\")\n","else:\n","    if (tpr >= 0.95) and (tnr >= 0.95):\n","        task4_marks = 2\n","    else:\n","        task4_marks = 0\n","\n","print(f\"\\n******* Your marks for Optional Task coding part is {task4_marks}/2. *******\\n\")"],"metadata":{"id":"rWKWOAfpbzE2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Briefly describe how you implement the watermarks. (1 mark)\n","\n","Your answer:"],"metadata":{"id":"HfVers__egb_"}},{"cell_type":"markdown","source":["# Submission\n","\n","After running all cells in this notebook, click File -> Download -> Download .ipynb to save this notebook as \"assignment2_CSIT375.ipynb\" locally.\n","Please open this file with Colab again to confirm that all the results are correctly shown.\n","\n","For submission, do not zip your entire project folder. You only need the following 5 files:\n","- assignment2_CSIT375.ipynb\n","- train_bad_module.py\n","- reverse_engineer.py\n","- adaptive_attack.py\n","- watermark.py\n","\n","Zip these files into a single zip file (do NOT use .rar). Submit this zip file via Moodle by the due date and time. Assignments that are not submitted on Moodle will not be marked."],"metadata":{"id":"YZFRuGOeLtUz"}}]}